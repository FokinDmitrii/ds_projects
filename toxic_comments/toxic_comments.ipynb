{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li></span></li></ul></li><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a </span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import notebook\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    " \n",
    "from sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.dummy import DummyRegressor\n",
    " \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,  make_scorer\n",
    " \n",
    "from pymystem3 import Mystem\n",
    "import re \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    " \n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords') \n",
    "#from sklearn.feature_extraction.text import CountVectorizer \n",
    " \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    " \n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "0       Explanation\\nWhy the edits made under my usern...      0\n",
       "1       D'aww! He matches this background colour I'm s...      0\n",
       "2       Hey man, I'm really not trying to edit war. It...      0\n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4       You, sir, are my hero. Any chance you remember...      0\n",
       "...                                                   ...    ...\n",
       "159566  \":::::And for the second time of asking, when ...      0\n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0\n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0\n",
       "159569  And it looks like it was actually you who put ...      0\n",
       "159570  \"\\nAnd ... I really don't think you understand...      0\n",
       "\n",
       "[159571 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = data.copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "проведем очистку текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):    \n",
    "    text = text.lower()\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemm_text = ' '.join([m.lemmatize(w) for w in word_list])\n",
    "    #lemm_text = \"\".join(m.lemmatize(text))\n",
    "    cleared_text = re.sub(r'[^a-zA-Z]', ' ', lemm_text) \n",
    "    return \" \".join(cleared_text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "проведем лемматизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lemmatize_text)\n",
    "target = df['toxic']\n",
    "features = df.drop(['toxic'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "разделим данные на тренировочные и тестовые"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features, \n",
    "                                                                              target, \n",
    "                                                                              test_size=0.4, \n",
    "                                                                              random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk_stopwords.words('english')) # \"стопслова\"\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "посчитаем Tf-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = count_tf_idf.fit_transform(features_train['text'].values)\n",
    "\n",
    "features_test = count_tf_idf.transform(features_test['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<95742x119712 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2605564 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовили данные, теперь обучим модель \"Дерева решений\" для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 761, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24/4235906801.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                        \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        verbose=True)\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtree_gs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtree_gs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_gs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \"\"\"\n\u001b[1;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    304\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \"\"\"\n\u001b[1;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1204\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(random_state = 123)\n",
    "param_grid = {\n",
    "   'decisiontreeclassifier__criterion':['gini', 'entropy'],        \n",
    "   'decisiontreeclassifier__max_depth':[4,6,10,12]\n",
    "}\n",
    "pip_gs = make_pipeline(TfidfVectorizer(stop_words=stopwords), tree) #with_mean=False\n",
    "tree_gs = GridSearchCV(pip_gs,\n",
    "                       param_grid=param_grid,\n",
    "                       cv=3, scoring='f1',\n",
    "                       verbose=True)\n",
    "tree_gs.fit(features_train, target_train)\n",
    "tree_gs.best_score_, tree_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат 0,6 не высокий и не удовлетворяет нашему условию >0,75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим работу модели Логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.64893422        nan 0.62816697        nan 0.62315647]\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.643044816455221"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(class_weight='balanced', random_state=123)\n",
    "params = {\n",
    "    'logisticregression__C': (0.1, 1, 10),\n",
    "    'logisticregression__penalty': ('l1', 'l2')\n",
    "}\n",
    "pip_lr = make_pipeline(StandardScaler(with_mean=False), lr)\n",
    "lr_gs = GridSearchCV(pip_lr, param_grid=params, cv=3, scoring='f1')\n",
    "\n",
    "lr_gs.fit(features_train, target_train)\n",
    "lr_gs.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Показатель F1 = 0,77 что вполне удовлетворяет поставленной задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим работу модели Случайного леса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.42183002638225997,\n",
       " {'randomforestclassifier__max_depth': 26,\n",
       "  'randomforestclassifier__n_estimators': 100,\n",
       "  'randomforestclassifier__random_state': 123})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf=RandomForestClassifier(class_weight='balanced', random_state=123)\n",
    "params = {\n",
    "    'randomforestclassifier__n_estimators': [10, 50, 100],\n",
    "    'randomforestclassifier__max_depth': [10, 12, 16, 20, 26],\n",
    "    'randomforestclassifier__random_state': [123]\n",
    "}\n",
    "pip_rf = make_pipeline(StandardScaler(with_mean=False), rf)\n",
    "rf_gs = GridSearchCV(pip_rf, param_grid=params, cv=5, scoring='f1').fit(features_train, target_train)\n",
    "rf_gs.best_score_, rf_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'standardscaler', 'randomforestclassifier', 'standardscaler__copy', 'standardscaler__with_mean', 'standardscaler__with_std', 'randomforestclassifier__bootstrap', 'randomforestclassifier__ccp_alpha', 'randomforestclassifier__class_weight', 'randomforestclassifier__criterion', 'randomforestclassifier__max_depth', 'randomforestclassifier__max_features', 'randomforestclassifier__max_leaf_nodes', 'randomforestclassifier__max_samples', 'randomforestclassifier__min_impurity_decrease', 'randomforestclassifier__min_impurity_split', 'randomforestclassifier__min_samples_leaf', 'randomforestclassifier__min_samples_split', 'randomforestclassifier__min_weight_fraction_leaf', 'randomforestclassifier__n_estimators', 'randomforestclassifier__n_jobs', 'randomforestclassifier__oob_score', 'randomforestclassifier__random_state', 'randomforestclassifier__verbose', 'randomforestclassifier__warm_start'])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_rf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4329589824095319"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.fit(features_train, target_train)\n",
    "rf_gs.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Точночть F1</th>\n",
       "      <th>параметры</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0,770</td>\n",
       "      <td>C=10, penalty=l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0,675</td>\n",
       "      <td>criterion=gini, max_depth=31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0,425</td>\n",
       "      <td>max_depth=26, n_estimators=100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Точночть F1                       параметры\n",
       "LogisticRegression           0,770                C=10, penalty=l2\n",
       "DecisionTreeClassifier       0,675    criterion=gini, max_depth=31\n",
       "RandomForestClassifier       0,425  max_depth=26, n_estimators=100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result=pd.DataFrame([{'Точночть F1':'0,770', 'параметры':'C=10, penalty=l2'}\n",
    "                     , {'Точночть F1':'0,675', 'параметры':'criterion=gini, max_depth=31'}\n",
    "                    , {'Точночть F1':'0,425', 'параметры':'max_depth=26, n_estimators=100'}],\n",
    "                    index=['LogisticRegression', 'DecisionTreeClassifier', 'RandomForestClassifier'])\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**\n",
    "- провели очистку данных и лемматизацию\n",
    "- убрали \"стопслова\" и подсчитали Tf-IDF текстов\n",
    "- разделили данные на тренировочные и тестовые в соотношении 60% и 40%\n",
    "- проверили работу 3-х моделей: \"Логистическая регрессия\", \"Дерево решений\" и \"Случайный лес\"\n",
    "Модель `LogisticRegression` показала лучший результат качества.\n",
    "С параметрами: 'C': 10, 'penalty': 'l2'\n",
    "\n",
    "эта модель будет давать наиболее точные предсказания позитивных и негативных комментариев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 12666,
    "start_time": "2022-05-25T07:14:38.137Z"
   },
   {
    "duration": 2321,
    "start_time": "2022-05-25T07:14:50.805Z"
   },
   {
    "duration": 17,
    "start_time": "2022-05-25T07:14:53.129Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-25T07:15:08.727Z"
   },
   {
    "duration": 4582,
    "start_time": "2022-05-27T13:40:11.311Z"
   },
   {
    "duration": 316,
    "start_time": "2022-05-27T13:40:56.016Z"
   },
   {
    "duration": 3517,
    "start_time": "2022-05-27T13:41:03.530Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-27T13:41:08.007Z"
   },
   {
    "duration": 24,
    "start_time": "2022-05-27T13:41:27.211Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-27T13:41:59.158Z"
   },
   {
    "duration": 1393,
    "start_time": "2022-05-27T13:42:35.483Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-27T13:43:04.191Z"
   },
   {
    "duration": 27,
    "start_time": "2022-05-27T13:45:10.763Z"
   },
   {
    "duration": 14,
    "start_time": "2022-05-27T13:47:34.395Z"
   },
   {
    "duration": 8,
    "start_time": "2022-05-27T13:47:49.899Z"
   },
   {
    "duration": 97,
    "start_time": "2022-05-27T13:49:10.192Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-27T13:49:48.635Z"
   },
   {
    "duration": 11,
    "start_time": "2022-05-27T13:50:04.800Z"
   },
   {
    "duration": 8,
    "start_time": "2022-05-27T13:50:15.187Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-27T13:50:30.861Z"
   },
   {
    "duration": 7,
    "start_time": "2022-05-27T13:50:37.109Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-27T13:53:38.754Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-27T13:53:43.528Z"
   },
   {
    "duration": 86,
    "start_time": "2022-05-27T13:53:44.932Z"
   },
   {
    "duration": 330,
    "start_time": "2022-05-27T13:53:48.876Z"
   },
   {
    "duration": 407,
    "start_time": "2022-05-27T13:56:18.805Z"
   },
   {
    "duration": 864,
    "start_time": "2022-05-27T13:58:13.129Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-27T13:58:15.258Z"
   },
   {
    "duration": 45,
    "start_time": "2022-05-27T13:58:16.971Z"
   },
   {
    "duration": 28,
    "start_time": "2022-05-27T13:58:20.896Z"
   },
   {
    "duration": 122832,
    "start_time": "2022-05-27T13:58:22.355Z"
   },
   {
    "duration": 53,
    "start_time": "2022-05-27T14:02:17.696Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-27T14:02:18.799Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-27T14:03:01.408Z"
   },
   {
    "duration": 12427,
    "start_time": "2022-05-27T14:03:05.591Z"
   },
   {
    "duration": 123839,
    "start_time": "2022-05-27T14:03:23.095Z"
   },
   {
    "duration": 39,
    "start_time": "2022-05-27T14:05:55.471Z"
   },
   {
    "duration": 29,
    "start_time": "2022-05-27T14:05:56.855Z"
   },
   {
    "duration": 6668,
    "start_time": "2022-05-27T14:05:57.999Z"
   },
   {
    "duration": 8,
    "start_time": "2022-05-27T14:06:27.920Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-27T14:06:30.486Z"
   },
   {
    "duration": 24,
    "start_time": "2022-05-27T14:06:31.920Z"
   },
   {
    "duration": 793,
    "start_time": "2022-05-27T14:06:34.390Z"
   },
   {
    "duration": 3584,
    "start_time": "2022-05-27T14:06:39.010Z"
   },
   {
    "duration": 3446,
    "start_time": "2022-05-27T14:07:09.602Z"
   },
   {
    "duration": 3530,
    "start_time": "2022-05-27T14:07:40.114Z"
   },
   {
    "duration": 8626,
    "start_time": "2022-05-27T14:07:56.774Z"
   },
   {
    "duration": 15070,
    "start_time": "2022-05-27T14:08:16.940Z"
   },
   {
    "duration": 16,
    "start_time": "2022-05-27T14:20:10.477Z"
   },
   {
    "duration": 18,
    "start_time": "2022-05-27T14:20:38.886Z"
   },
   {
    "duration": 5452,
    "start_time": "2022-05-27T14:20:51.397Z"
   },
   {
    "duration": 44575,
    "start_time": "2022-05-27T14:22:59.793Z"
   },
   {
    "duration": 117693,
    "start_time": "2022-05-27T14:24:25.359Z"
   },
   {
    "duration": 90060,
    "start_time": "2022-05-27T14:26:46.841Z"
   },
   {
    "duration": 17,
    "start_time": "2022-05-27T14:31:40.801Z"
   },
   {
    "duration": 17,
    "start_time": "2022-05-27T14:32:36.213Z"
   },
   {
    "duration": 17,
    "start_time": "2022-05-27T14:34:45.297Z"
   },
   {
    "duration": 8,
    "start_time": "2022-05-27T14:35:20.874Z"
   },
   {
    "duration": 20,
    "start_time": "2022-05-27T14:37:42.269Z"
   },
   {
    "duration": 34202,
    "start_time": "2022-05-27T14:39:03.247Z"
   },
   {
    "duration": 106018,
    "start_time": "2022-05-27T16:27:42.313Z"
   },
   {
    "duration": 369,
    "start_time": "2022-05-28T05:38:31.495Z"
   },
   {
    "duration": 2800,
    "start_time": "2022-05-28T05:38:34.723Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-28T05:38:37.525Z"
   },
   {
    "duration": 19,
    "start_time": "2022-05-28T05:38:44.549Z"
   },
   {
    "duration": 25,
    "start_time": "2022-05-28T05:38:49.143Z"
   },
   {
    "duration": 906,
    "start_time": "2022-05-28T05:39:00.722Z"
   },
   {
    "duration": 7,
    "start_time": "2022-05-28T05:39:30.001Z"
   },
   {
    "duration": 31,
    "start_time": "2022-05-28T05:39:47.045Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-28T05:41:38.404Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-28T05:41:40.916Z"
   },
   {
    "duration": 41,
    "start_time": "2022-05-28T05:41:47.053Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-28T05:42:36.779Z"
   },
   {
    "duration": 385,
    "start_time": "2022-05-28T05:42:50.927Z"
   },
   {
    "duration": 18,
    "start_time": "2022-05-28T05:46:22.197Z"
   },
   {
    "duration": 786,
    "start_time": "2022-05-28T05:47:26.845Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-28T05:47:27.748Z"
   },
   {
    "duration": 32,
    "start_time": "2022-05-28T05:47:35.550Z"
   },
   {
    "duration": 26,
    "start_time": "2022-05-28T05:47:37.552Z"
   },
   {
    "duration": 82495,
    "start_time": "2022-05-28T05:47:40.280Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-28T05:52:21.677Z"
   },
   {
    "duration": 20,
    "start_time": "2022-05-28T05:52:29.554Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-28T05:53:08.398Z"
   },
   {
    "duration": 19,
    "start_time": "2022-05-28T05:53:10.426Z"
   },
   {
    "duration": 26,
    "start_time": "2022-05-28T05:53:51.973Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-28T05:53:55.102Z"
   },
   {
    "duration": 10266,
    "start_time": "2022-05-28T05:53:58.058Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-28T05:54:11.516Z"
   },
   {
    "duration": 341592,
    "start_time": "2022-05-28T05:54:38.497Z"
   },
   {
    "duration": 255692,
    "start_time": "2022-05-28T06:01:32.259Z"
   },
   {
    "duration": 716255,
    "start_time": "2022-05-28T06:07:42.638Z"
   },
   {
    "duration": 891882,
    "start_time": "2022-05-28T06:32:50.110Z"
   },
   {
    "duration": 921070,
    "start_time": "2022-05-28T06:55:54.961Z"
   },
   {
    "duration": 12,
    "start_time": "2022-05-28T07:11:16.033Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-28T07:23:27.479Z"
   },
   {
    "duration": 9,
    "start_time": "2022-05-28T07:23:59.862Z"
   },
   {
    "duration": 289,
    "start_time": "2022-05-29T08:28:06.526Z"
   },
   {
    "duration": 255,
    "start_time": "2022-05-29T08:28:58.728Z"
   },
   {
    "duration": 708,
    "start_time": "2022-05-29T08:29:11.722Z"
   },
   {
    "duration": 8,
    "start_time": "2022-05-29T08:29:13.224Z"
   },
   {
    "duration": 42,
    "start_time": "2022-05-29T08:29:18.670Z"
   },
   {
    "duration": 8,
    "start_time": "2022-05-29T08:35:56.372Z"
   },
   {
    "duration": 35,
    "start_time": "2022-05-29T08:55:29.537Z"
   },
   {
    "duration": 7086,
    "start_time": "2022-05-29T08:55:33.065Z"
   },
   {
    "duration": 4889,
    "start_time": "2022-05-29T08:58:07.569Z"
   },
   {
    "duration": 25,
    "start_time": "2022-05-29T09:02:50.062Z"
   },
   {
    "duration": 13,
    "start_time": "2022-05-29T09:02:53.100Z"
   },
   {
    "duration": 6685,
    "start_time": "2022-05-29T09:02:58.356Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-29T09:03:05.043Z"
   },
   {
    "duration": 24,
    "start_time": "2022-05-29T09:42:07.209Z"
   },
   {
    "duration": 22,
    "start_time": "2022-05-29T09:43:31.780Z"
   },
   {
    "duration": 23,
    "start_time": "2022-05-29T09:44:33.812Z"
   },
   {
    "duration": 20,
    "start_time": "2022-05-29T09:45:51.426Z"
   },
   {
    "duration": 49,
    "start_time": "2022-05-29T09:48:20.528Z"
   },
   {
    "duration": 40,
    "start_time": "2022-05-29T09:56:51.396Z"
   },
   {
    "duration": 41,
    "start_time": "2022-05-29T10:00:49.590Z"
   },
   {
    "duration": 52,
    "start_time": "2022-05-29T10:01:29.473Z"
   },
   {
    "duration": 41,
    "start_time": "2022-05-29T10:05:25.996Z"
   },
   {
    "duration": 52,
    "start_time": "2022-05-29T10:07:28.118Z"
   },
   {
    "duration": 49,
    "start_time": "2022-05-29T10:12:40.136Z"
   },
   {
    "duration": 23,
    "start_time": "2022-05-29T10:21:27.739Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-29T10:21:42.964Z"
   },
   {
    "duration": 44,
    "start_time": "2022-05-29T10:22:32.668Z"
   },
   {
    "duration": 351,
    "start_time": "2022-05-29T10:23:06.250Z"
   },
   {
    "duration": 24,
    "start_time": "2022-05-29T10:25:20.496Z"
   },
   {
    "duration": 26,
    "start_time": "2022-05-29T10:25:47.386Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-29T10:27:05.321Z"
   },
   {
    "duration": 280,
    "start_time": "2022-05-29T10:28:01.628Z"
   },
   {
    "duration": 403,
    "start_time": "2022-05-29T10:28:33.341Z"
   },
   {
    "duration": 234,
    "start_time": "2022-05-29T10:30:29.097Z"
   },
   {
    "duration": 25,
    "start_time": "2022-05-29T10:31:33.906Z"
   },
   {
    "duration": 364,
    "start_time": "2022-05-29T10:41:41.080Z"
   },
   {
    "duration": 273301,
    "start_time": "2022-05-29T11:12:36.700Z"
   },
   {
    "duration": 513152,
    "start_time": "2022-05-29T11:17:32.642Z"
   },
   {
    "duration": 72,
    "start_time": "2022-05-29T11:31:07.348Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-29T11:32:04.816Z"
   },
   {
    "duration": 1200632,
    "start_time": "2022-05-29T11:33:11.340Z"
   },
   {
    "duration": 1119902,
    "start_time": "2022-05-29T11:55:22.411Z"
   },
   {
    "duration": 4401,
    "start_time": "2022-05-30T12:18:42.569Z"
   },
   {
    "duration": 2243,
    "start_time": "2022-05-30T12:18:51.976Z"
   },
   {
    "duration": 12,
    "start_time": "2022-05-30T12:18:54.221Z"
   },
   {
    "duration": 32,
    "start_time": "2022-05-30T12:19:00.335Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-30T12:24:34.259Z"
   },
   {
    "duration": 143017,
    "start_time": "2022-05-30T12:24:40.049Z"
   },
   {
    "duration": 30,
    "start_time": "2022-05-30T12:27:22.852Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-30T12:27:24.249Z"
   },
   {
    "duration": 7132,
    "start_time": "2022-05-30T12:27:27.447Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-30T12:27:40.913Z"
   },
   {
    "duration": 160839,
    "start_time": "2022-05-30T12:27:45.684Z"
   },
   {
    "duration": 612,
    "start_time": "2022-05-30T12:35:44.682Z"
   },
   {
    "duration": 23,
    "start_time": "2022-05-30T12:37:01.599Z"
   },
   {
    "duration": 315,
    "start_time": "2022-05-30T12:37:46.550Z"
   },
   {
    "duration": 290,
    "start_time": "2022-05-30T12:39:23.016Z"
   },
   {
    "duration": 24,
    "start_time": "2022-05-30T12:39:39.551Z"
   },
   {
    "duration": 28,
    "start_time": "2022-05-30T12:44:52.849Z"
   },
   {
    "duration": 29,
    "start_time": "2022-05-30T12:45:51.511Z"
   },
   {
    "duration": 21,
    "start_time": "2022-05-30T12:49:07.656Z"
   },
   {
    "duration": 18,
    "start_time": "2022-05-30T12:49:34.633Z"
   },
   {
    "duration": 8025,
    "start_time": "2022-05-31T08:21:10.927Z"
   },
   {
    "duration": 2194,
    "start_time": "2022-05-31T08:21:18.954Z"
   },
   {
    "duration": 13,
    "start_time": "2022-05-31T08:21:21.150Z"
   },
   {
    "duration": 35,
    "start_time": "2022-05-31T08:21:24.734Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-31T08:21:28.668Z"
   },
   {
    "duration": 125438,
    "start_time": "2022-05-31T08:21:34.246Z"
   },
   {
    "duration": 29,
    "start_time": "2022-05-31T08:47:49.455Z"
   },
   {
    "duration": 8,
    "start_time": "2022-05-31T08:47:50.324Z"
   },
   {
    "duration": 6632,
    "start_time": "2022-05-31T08:47:52.930Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-31T08:47:59.564Z"
   },
   {
    "duration": 609,
    "start_time": "2022-05-31T08:48:04.574Z"
   },
   {
    "duration": 24,
    "start_time": "2022-05-31T08:48:37.686Z"
   },
   {
    "duration": 19,
    "start_time": "2022-05-31T08:50:51.460Z"
   },
   {
    "duration": 21,
    "start_time": "2022-05-31T08:51:36.825Z"
   },
   {
    "duration": 24,
    "start_time": "2022-05-31T08:52:06.913Z"
   },
   {
    "duration": 59,
    "start_time": "2022-05-31T08:54:41.293Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-31T08:55:54.872Z"
   },
   {
    "duration": 49,
    "start_time": "2022-05-31T08:59:27.546Z"
   },
   {
    "duration": 52,
    "start_time": "2022-05-31T08:59:54.277Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-31T11:12:46.087Z"
   },
   {
    "duration": 24,
    "start_time": "2022-05-31T11:13:27.651Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-31T11:14:38.436Z"
   },
   {
    "duration": 43,
    "start_time": "2022-05-31T11:14:44.096Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-31T11:15:12.147Z"
   },
   {
    "duration": 55,
    "start_time": "2022-05-31T11:15:12.671Z"
   },
   {
    "duration": 73,
    "start_time": "2022-05-31T11:16:06.695Z"
   },
   {
    "duration": 45,
    "start_time": "2022-05-31T11:17:11.649Z"
   },
   {
    "duration": 51,
    "start_time": "2022-05-31T11:18:17.857Z"
   },
   {
    "duration": 27,
    "start_time": "2022-05-31T12:37:12.633Z"
   },
   {
    "duration": 25,
    "start_time": "2022-05-31T12:37:51.189Z"
   },
   {
    "duration": 309,
    "start_time": "2022-05-31T12:38:47.114Z"
   },
   {
    "duration": 286,
    "start_time": "2022-05-31T12:40:20.327Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
